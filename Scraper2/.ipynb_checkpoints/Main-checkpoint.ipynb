{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.item import Item, Field\n",
    "from items import WebPageItem, WebPolicyItem, ExportItem\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.loader import ItemLoader\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import sys\n",
    "from scrapy_splash import SplashRequest\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "from scrapy.utils.project import get_project_settings\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class finalSpider(scrapy.Spider):\n",
    "  # the name is required to run the spider from the command line\n",
    "  name = \"refined\"\n",
    "    \n",
    "#   def __init__(self, origin='', **kwargs):\n",
    "#     self.first_url = [f'{origin}']  \n",
    "#     super().__init__(**kwargs)\n",
    "\n",
    "  # eventually this start_urls will need to contain the url from the user's active tab in Chrome\n",
    "    # we are planning to use the chrome api chrome.activeTab.url and store the js variable\n",
    "    # so we need to find a way to use a js variable or to directly call the url of the current tab\n",
    "#   start_urls = [\n",
    "#     'http://www.amazon.com'\n",
    "#     # sys.argv[1]\n",
    "#       ]\n",
    "  def start_requests(self):\n",
    "    yield SplashRequest(\n",
    "    url=self.origin_url,\n",
    "    callback=self.parse,\n",
    "    args={'wait': 2, 'viewport': '1024x2480', 'timeout': 90, 'images': 0, 'resource_timeout': 15},\n",
    "      )\n",
    "\n",
    "\n",
    "#   date_updated = \"07-04-1997\"\n",
    "\n",
    "  #print(sys.argv[1])\n",
    "#   custom_settings = {'FEED_FORMAT':'json', 'FEED_URI':'EC2testoutput.json'}\n",
    "\n",
    "  custom_settings = {'FEEDS':{\n",
    "                              'scraped_data.json': {'format': 'json'}\n",
    "                              },\n",
    "                    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def validate(self, date_text):\n",
    "        try:\n",
    "            new_date = datetime.datetime.strptime(date_text, '%B %d, %Y')\n",
    "            return new_date\n",
    "        except ValueError:\n",
    "            raise ValueError(\"Incorrect data format, should be something else\")\n",
    "            try:\n",
    "                new_date = datetime.datetime.strptime(date_text, '%b %d, %Y')\n",
    "                return new_date\n",
    "            except ValueError:\n",
    "            raise ValueError(\"Incorrect data format, should be something else\")\n",
    "            try:\n",
    "              new_date = datetime.datetime.strptime(date_text, '%B %Y')\n",
    "              return new_date\n",
    "            except ValueError:\n",
    "              raise ValueError(\"Incorrect data format, should be something else\")\n",
    "\n",
    "\n",
    "    def scrape_interesting_links(self, response):\n",
    "        print('\\n', '------------- IN THE SCOPE OF THE FIRST METHOD -------------')\n",
    "\n",
    "        next_links = []\n",
    "\n",
    "        privacy_links = []\n",
    "        privacy_items = []\n",
    "\n",
    "        terms_links = []\n",
    "        terms_items = []\n",
    "\n",
    "        conditions_links = []\n",
    "        conditions_items = []\n",
    "\n",
    "        cookies_links = []\n",
    "        cookies_items = []\n",
    "\n",
    "\n",
    "        p_perfect, t_perfect, cond_perfect, cook_perfect = (0,0,0,0)\n",
    "\n",
    "\n",
    "        ## GO THROUGH ALL LINKS AND CATEGORIZE THEM, PICKING OUT PERFECT LINKS\n",
    "        for item in response.css('a'):\n",
    "          # print('LINK ITEM', item.css('a::attr(href)').get())\n",
    "\n",
    "          ## PRIVACY\n",
    "            if len(item.css('a *::text').re(r'.rivacy')) != 0: \n",
    "                print('\\n')\n",
    "            if p_perfect == 1:\n",
    "                continue\n",
    "\n",
    "\n",
    "        else:\n",
    "            p_link = item.css('a::attr(href)').get()\n",
    "\n",
    "            if any(p_link in sublist for sublist in [privacy_links, terms_links, conditions_links, cookies_links]):\n",
    "                print('THIS IS A REPEAT PRIVACY LINK: ', p_link)\n",
    "                print('PRIVACY ITEMS: ', privacy_items)\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            if 'privacy' in p_link[-8:]:\n",
    "                print('PERFECT PRIVACY LINK', p_link)\n",
    "                p_perfect = 1\n",
    "\n",
    "                # next_links.append(item.css('a::attr(href)').get())\n",
    "                next_links.append(p_link)\n",
    "                privacy_links.append(p_link)\n",
    "\n",
    "                privacy_item = WebPolicyItem()\n",
    "                # privacy_item['label'] = item.css('a *::text').get()\n",
    "                # privacy_item['link'] = item.css('a::attr(href)').get()\n",
    "                privacy_item['link'] = p_link\n",
    "\n",
    "                # privacy_items.append(privacy_item)\n",
    "                # self.domain[response.url]['privacy'] = privacy_item\n",
    "                # self.domain[response.url]['privacy'].append(privacy_item)\n",
    "                privacy_items = [privacy_item]\n",
    "                print('PRIVACY ITEMS AFTER PERFECT PRIVACY LINK: ', privacy_items)\n",
    "\n",
    "        else:\n",
    "            privacy_links.append(p_link)\n",
    "            print('PRIVACY LINK APPENDED', p_link)\n",
    "\n",
    "            privacy_item = WebPolicyItem()\n",
    "            # privacy_item['label'] = item.css('a *::text').get()\n",
    "            privacy_item['link'] = p_link\n",
    "\n",
    "            privacy_items.append(privacy_item)\n",
    "\n",
    "            print('PRIVACY ITEMS AFTER IMPERFECT PRIVACY LINK: ', privacy_items)\n",
    "            \n",
    "\n",
    "\n",
    "      ## TERMS   \n",
    "      if len(item.css('a *::text').re(r'.erms')) != 0: \n",
    "        print('\\n') \n",
    "        if t_perfect == 1:\n",
    "          # terms_items = [{'link': t_link}]\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            t_link = item.css('a::attr(href)').get()\n",
    "\n",
    "        if any(t_link in sublist for sublist in [privacy_links, terms_links, conditions_links, cookies_links]):\n",
    "            print('THIS IS A REPEAT TERMS LINK: ', t_link)\n",
    "            print('TERMS ITEMS: ', terms_items)\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            if 'terms' in t_link[-6:]:\n",
    "                print('PERFECT TERMS LINK', t_link)\n",
    "                t_perfect = 1\n",
    "\n",
    "                # # need to comment this line in\n",
    "                next_links.append(t_link)\n",
    "                terms_links.append(t_link)\n",
    "\n",
    "                terms_item = WebPolicyItem()\n",
    "                # terms_item['label'] = item.css('a *::text').get()\n",
    "                terms_item['link'] = t_link\n",
    "\n",
    "                # # need to comment this line out\n",
    "                # terms_items.append(terms_item)\n",
    "                # self.domain[response.url]['terms'] = terms_item\n",
    "\n",
    "                # # need to comment this line in\n",
    "                terms_items = [terms_item]\n",
    "                print('TERMS ITEMS AFTER PERFECT TERMS LINK: ', terms_items)\n",
    "\n",
    "            else:\n",
    "                terms_links.append(t_link)\n",
    "                print('TERMS LINK APPENDED', t_link)\n",
    "\n",
    "                terms_item = WebPolicyItem()\n",
    "                # terms_item['label'] = item.css('a *::text').get()\n",
    "                terms_item['link'] = t_link\n",
    "\n",
    "                terms_items.append(terms_item)\n",
    "\n",
    "                print('TERMS ITEMS AFTER IMPERFECT TERMS LINK: ', terms_items)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "      ## CONDITIONS\n",
    "      if len(item.css('a *::text').re(r'.onditions')) != 0:\n",
    "        print('\\n')\n",
    "        if cond_perfect == 1:\n",
    "          continue\n",
    "\n",
    "        else:\n",
    "          cond_link = item.css('a::attr(href)').get()\n",
    "\n",
    "          if any(cond_link in sublist for sublist in [privacy_links, terms_links, conditions_links, cookies_links]):\n",
    "            print('THIS IS A REPEAT CONDITIONS LINK: ', cond_link)\n",
    "            print('CONDITIONS ITEMS: ', conditions_items)\n",
    "            continue\n",
    "\n",
    "          else:\n",
    "            if 'conditions' in cond_link[-11:]:\n",
    "              print('PERFECT CONDITIONS LINK', cond_link)\n",
    "              cond_perfect = 1\n",
    "\n",
    "              next_links.append(cond_link)\n",
    "              conditions_links.append(cond_link)\n",
    "\n",
    "              conditions_item = WebPolicyItem()\n",
    "              # conditions_item['label'] = item.css('a *::text').get()\n",
    "              conditions_item['link'] = cond_link\n",
    "\n",
    "              # conditions_items.append(conditions_item)\n",
    "              # self.domain[response.url]['conditions'] = conditions_item\n",
    "              conditions_items = [conditions_item]\n",
    "              print('CONDITIONS ITEMS AFTER PERFECT CONDITIONS LINK: ', conditions_items)\n",
    "\n",
    "            else:\n",
    "              conditions_links.append(cond_link)\n",
    "              print('CONDITIONS LINK APPENDED', cond_link)\n",
    "\n",
    "              conditions_item = WebPolicyItem()\n",
    "              # conditions_item['label'] = item.css('a *::text').get()\n",
    "              conditions_item['link'] = cond_link\n",
    "\n",
    "              conditions_items.append(conditions_item)\n",
    "\n",
    "              print('CONDITIONS ITEMS AFTER IMPERFECT CONDITIONS LINK: ', conditions_items)\n",
    "\n",
    "            # if cond_link not in conditions_links:\n",
    "            #   conditions_links.append(item.css('a::attr(href)').get())\n",
    "            #   print('CONDITIONS LINK APPENDED', cond_link)\n",
    "\n",
    "            # if 'conditions_item' in locals():\n",
    "            #   continue\n",
    "\n",
    "            # else:\n",
    "            #   conditions_item = WebPolicyItem()\n",
    "            #   self.domain[response.url]['conditions'] = conditions_item\n",
    "\n",
    "\n",
    "\n",
    "      ## COOKIES\n",
    "      if len(item.css('a *::text').re(r'.ookies')) != 0: \n",
    "        print('\\n')\n",
    "        if cook_perfect == 1:\n",
    "          continue\n",
    "\n",
    "        else:\n",
    "          cook_link = item.css('a::attr(href)').get()\n",
    "\n",
    "          if any(cook_link in sublist for sublist in [privacy_links, terms_links, conditions_links, cookies_links]):\n",
    "            print('THIS IS A REPEAT COOKIES LINK: ', cook_link)\n",
    "            print('COOKIES ITEMS: ', cookies_items)\n",
    "            continue\n",
    "\n",
    "          else:\n",
    "            if 'cookie' in cook_link[-8:]:\n",
    "              print('PERFECT COOKIES LINK', cook_link)\n",
    "              cook_perfect = 1\n",
    "\n",
    "              next_links.append(cook_link)\n",
    "              cookies_links.append(cook_link)\n",
    "\n",
    "              cookies_item = WebPolicyItem()\n",
    "              # privacy_item['label'] = item.css('a *::text').get()\n",
    "              cookies_item['link'] = cook_link\n",
    "\n",
    "              # cookies_items.append(cookies_item)\n",
    "\n",
    "              # self.domain[response.url]['cookies'] = cookies_item\n",
    "              # self.domain[response.url]['cookies'].append(cookies_item)\n",
    "\n",
    "              cookies_items = [cookies_item]\n",
    "              print('COOKIES ITEMS AFTER PERFECT COOKIES LINK: ', cookies_items)\n",
    "\n",
    "\n",
    "            else:\n",
    "              cookies_links.append(cook_link)\n",
    "              print('COOKIES LINK APPENDED', cook_link)\n",
    "\n",
    "              cookies_item = WebPolicyItem()\n",
    "              # cookies_item['label'] = item.css('a *::text').get()\n",
    "              cookies_item['link'] = cook_link\n",
    "\n",
    "              cookies_items.append(cookies_item)\n",
    "\n",
    "              print('COOKIES ITEMS AFTER IMPERFECT COOKIES LINK: ', cookies_items)\n",
    "\n",
    "\n",
    "\n",
    "    # # # PEEPING STUFF\n",
    "\n",
    "    print('\\n', 'PRIVACY LINKS', privacy_links)\n",
    "    print('PRIVACY ITEMS', privacy_items, '\\n')\n",
    "\n",
    "    print('TERMS LINKS', terms_links)\n",
    "    print('TERMS ITEMS', terms_items, '\\n')\n",
    "    \n",
    "    print('CONDITIONS LINKS', conditions_links)\n",
    "    print('CONDITIONS ITEMS', conditions_items, '\\n')\n",
    "    \n",
    "    print('COOKIES LINKS', cookies_links)\n",
    "    print('COOKIES ITEMS', cookies_items, '\\n')\n",
    "\n",
    "\n",
    "    print('NEXT LINKS', next_links)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # # # START sort_interesting_links METHOD\n",
    "    print('\\n', '------------ STARTING SORTING PROCESS NOW -----------')\n",
    "\n",
    "    base_url = list(self.domain.keys())[0]\n",
    "    print('BASE URL', base_url)\n",
    "\n",
    "    inner_object = self.domain[base_url]\n",
    "    print('WITHIN BASE_URL OBJECT', inner_object, '\\n')\n",
    "    #  # this is an object with four categories as keys and four empty lists as values\n",
    "   \n",
    "\n",
    "\n",
    "    if len(privacy_items) == 0:\n",
    "      print('EMPTY PRIVACY ITEMS LIST')\n",
    "    elif len(privacy_items) == 1:\n",
    "      print('PRIVACY ITEMS LIST OF LENGTH 1')\n",
    "      inner_object['privacy'].append(privacy_items[0])\n",
    "      if privacy_items[0]['link'] not in next_links:\n",
    "        next_links.append(privacy_items[0]['link'])\n",
    "    else:\n",
    "      print('MULTIPLE PRIVACY ITEMS')\n",
    "      # self.link_parse(privacy_links)\n",
    "      for item in privacy_items:\n",
    "        print('ITEM: ', item)\n",
    "\n",
    "        next_page = response.urljoin(item['link'])\n",
    "        print('PRIVACY POLICY LINK CANDIDATE:', next_page)\n",
    "        # if item['link'] in next_links:\n",
    "        #   inner_object['privacy'].append(item)\n",
    "\n",
    "        # else:\n",
    "        #   next_page = response.urljoin(item['link'])\n",
    "        #   print('PRIVACY POLICY LINK CANDIDATE:', next_page)\n",
    "\n",
    "        # self.link_parse(next_page)\n",
    "        # yield scrapy.Request(next_page, callback=self.link_parse) \n",
    "\n",
    "\n",
    "\n",
    "    if len(terms_items) == 0:\n",
    "      print('EMPTY TERMS ITEMS LIST')\n",
    "    elif len(terms_items) == 1:\n",
    "      print('TERMS ITEMS LIST OF LENGTH 1')\n",
    "      inner_object['terms'].append(terms_items[0])\n",
    "      if terms_items[0]['link'] not in next_links:\n",
    "        next_links.append(terms_items[0]['link'])\n",
    "    else:\n",
    "      print('MULTIPLE TERMS ITEMS')\n",
    "      # self.link_parse(privacy_links)\n",
    "\n",
    "      for item in terms_items:\n",
    "        print('ITEM: ', item)\n",
    "\n",
    "        next_page = response.urljoin(item['link'])\n",
    "        print('TERMS POLICY LINK CANDIDATE:', next_page)\n",
    "        \n",
    "        # if item['link'] in next_links:\n",
    "        #   inner_object['terms'].append(item)\n",
    "        # else:\n",
    "        #   next_page = response.urljoin(item['link'])\n",
    "        #   print('TERMS POLICY LINK CANDIDATES:', next_page)\n",
    "\n",
    "          # do we want to keep all the potential link\n",
    "          # inner_object['terms'].append(item)\n",
    "\n",
    "\n",
    "\n",
    "    if len(conditions_items) == 0:\n",
    "      print('EMPTY CONDITIONS ITEMS LIST')\n",
    "    elif len(conditions_items) == 1:\n",
    "      print('CONDITIONS ITEMS LIST OF LENGTH 1')\n",
    "      inner_object['conditions'].append(conditions_items[0])\n",
    "      if conditions_items[0]['link'] not in next_links:\n",
    "        next_links.append(conditions_items[0]['link'])\n",
    "    else:\n",
    "      print('MULTIPLE CONDITIONS ITEMS')\n",
    "      # self.link_parse(privacy_links)\n",
    "      for item in conditions_items:\n",
    "        print('ITEM: ', item)\n",
    "\n",
    "        next_page = response.urljoin(item['link'])\n",
    "        print('CONDITIONS POLICY LINK CANDIDATE:', next_page)\n",
    "          # if item['link'] in next_links:\n",
    "        #   inner_object['privacy'].append(item)\n",
    "\n",
    "        # else:\n",
    "        #   next_page = response.urljoin(item['link'])\n",
    "        #   print('CONDITIONS POLICY LINK CANDIDATE:', next_page)\n",
    "\n",
    "\n",
    "\n",
    "    if len(cookies_items) == 0:\n",
    "      print('EMPTY COOKIES ITEMS LIST')\n",
    "    elif len(cookies_items) == 1:\n",
    "      print('COOKIES ITEMS LIST OF LENGTH 1')\n",
    "      inner_object['cookies'].append(conditions_items[0])\n",
    "      if cookies_items[0]['link'] not in next_links:\n",
    "        next_links.append(cookies_items[0]['link'])\n",
    "    else:\n",
    "      print('MULTIPLE COOKIES ITEMS')\n",
    "      # self.link_parse(privacy_links)\n",
    "      for item in cookies_items:\n",
    "        print('ITEM: ', item)\n",
    "\n",
    "        next_page = response.urljoin(item['link'])\n",
    "        print('COOKIES POLICY LINK CANDIDATE:', next_page)\n",
    "        # if item['link'] in next_links:\n",
    "        #   inner_object['cookies'].append(item)\n",
    "\n",
    "        # else:\n",
    "        #   next_page = response.urljoin(item['link'])\n",
    "        #   print('COOKIES POLICY LINK CANDIDATES:', next_page)\n",
    "          # yield scrapy.Request(next_page, callback=self.link_parse)  \n",
    "\n",
    "     \n",
    "\n",
    "    # for link_list in [privacy_links, terms_links, conditions_links]:\n",
    "    #   if len(link_list) == 1:\n",
    "    #     for link in link_list:\n",
    "    #       if link not in next_links:\n",
    "    #         next_links.append(link)\n",
    "\n",
    "    print('NEW NEXT LINKS', next_links)\n",
    "\n",
    "    print('INNER OBJECT WITHIN BASE URL KEY', inner_object)\n",
    "    self.domain[base_url] = inner_object\n",
    "\n",
    "    items_list = [privacy_items, terms_items, conditions_items, cookies_items]\n",
    "\n",
    "\n",
    "    return next_links, inner_object, items_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # this method is for scraping the text of the policy pages\n",
    "  def policy_scraper(self, response):\n",
    "     ## maybe we find the body text of the policy by searching by div elements\n",
    "          ## we can find the length of all the divs and try to pull from the longest one\n",
    "          ## but there might be a div that wraps the whole page \n",
    "    print('\\n', '-------- WE ARE INSIDE THE POLICY SCRAPER --------')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### START INTERNAL LOGIC TO GET MAIN BODY OF POLICY PAGE\n",
    "\n",
    "    # print(response.text)\n",
    "\n",
    "    div_h1_dict = {}\n",
    "    div_h2_dict = {}\n",
    "    div_h3_dict = {}\n",
    "    div_strong_dict = {}\n",
    "    max_div_h1_index = []\n",
    "    max_div_h2_index = []\n",
    "    max_div_h3_index = []\n",
    "    max_div_strong_index = []\n",
    "\n",
    "\n",
    "    if len(response.css('body div')) == 0:\n",
    "      print('THERE ARE NO DIVS IN THE BODY OF THIS PAGE')\n",
    "\n",
    "      last_updated_entry = ['No entry located']\n",
    "      \n",
    "      entry = False\n",
    "\n",
    "      regex = r'(.ast .pdated)'\n",
    "      regex2 = r'(.ast .odified)'\n",
    "      regex3 = r'(.olicy .ffective)'\n",
    "      regex4 = r'(.ffective .ate)'\n",
    "      regex5 = r'(.olicy .odified)'\n",
    "      regex6 = r'(.pdated:)'\n",
    "      regex7 = r'(.ffective:)'\n",
    "\n",
    "\n",
    "    # for index, tag in enumerate(response.css('*')):\n",
    "      for tag in response.css('*'):\n",
    "        # print(tag)\n",
    "        # print('ENTRY COUNT: ', len(last_updated_entry))\n",
    "        if last_updated_entry[0] != 'No entry located':\n",
    "          break\n",
    "\n",
    "        else:\n",
    "          for pattern in [regex, regex2, regex3, regex4, regex5, regex6, regex7]:\n",
    "            if re.search(pattern, str(tag.css('*::text').get())):\n",
    "              print('HOW MANY TEXT ITEMS IN THIS TAG: ', len(tag.css('*::text').getall()))\n",
    "\n",
    "              if len(tag.css('*::text').getall()) > 1:\n",
    "                # print(''.join(tag.css('*::text').getall()))\n",
    "                # last_updated_entry.append(''.join(tag.css('*::text').getall()))\n",
    "                last_updated_entry[0] = ''.join(tag.css('*::text').getall())\n",
    "                break\n",
    "            \n",
    "\n",
    "      if last_updated_entry[0] != 'No entry located':\n",
    "        update_entry = last_updated_entry[0]\n",
    "        update_entry = update_entry.split(':')[1].strip()\n",
    "        print('UPDATED ENTRY: ', update_entry)\n",
    "\n",
    "      else:\n",
    "        update_entry = last_updated_entry[0].strip()\n",
    "        print('UPDATED ENTRY: ', update_entry)\n",
    "\n",
    "\n",
    "      deepest_tag_tuple = (0,0)\n",
    "\n",
    "      for index,tag in enumerate(response.css('*')): \n",
    "        # print('tag index', index)\n",
    "        # print('tag', '\\n', len(tag.css('*')))\n",
    "        if index != 0:\n",
    "          this_tag_length = len(tag.css('*'))\n",
    "          if this_tag_length > deepest_tag_tuple[1]:\n",
    "            deepest_tag_tuple = (index, this_tag_length)\n",
    "\n",
    "      # print(deepest_tag_tuple)\n",
    "\n",
    "\n",
    "      lucky_tag = response.css('*')[deepest_tag_tuple[0]]\n",
    "      # print('The lucky tag has this many tags: ', len(lucky_tag.css('*')))\n",
    "\n",
    "\n",
    "      page_text_with_tags = ['start']\n",
    "      page_text = ['start']\n",
    "\n",
    "      for index,tag in enumerate(lucky_tag.css('*')):\n",
    "        # print(index, tag)\n",
    "\n",
    "\n",
    "        if '<script' not in tag.get() and '<style' not in tag.get() and '<option' not in tag.get():\n",
    "          # print(index)\n",
    "          # print('\\n', 'tag code:', '\\n', tag.get())\n",
    "          # print('\\n', 'tag text:', '\\n', tag.css('::text').getall())\n",
    "          # print(''.join(tag.css('::text').getall()))\n",
    "          # print('\\n', '\\n')\n",
    "\n",
    "          # regex = r'^.(.ast .pdated)'\n",
    "          # re.search(regex, tag.css('::text').get())\n",
    "\n",
    "          # # if len(tag.css(' ::text').re(r'.ast .pdated')) != 0:\n",
    "          # if re.search(regex, tag.css('::text').get()):\n",
    "          #   print('TEXT ABOUT LAST TIME THE POLICY WAS UPDATED: ', tag.css('::text').get())\n",
    "\n",
    "          if tag.get() not in page_text_with_tags[-1]:\n",
    "            ## this one still has the html wrappers within the outside tag\n",
    "            page_text_with_tags.append(tag.get())\n",
    "\n",
    "            \n",
    "            ## this one takes the text out of each tag\n",
    "            page_text.append(tag.css('::text').getall())\n",
    "            # page_text.append(tag.css('::text').get())\n",
    "        \n",
    "          # else:\n",
    "            # page_text[-1] += tag.css('::text').get()\n",
    "            \n",
    "      del(page_text_with_tags[0])\n",
    "      del(page_text[0])\n",
    "\n",
    "      joined_page_text = ' \\n'.join([''.join(i) for i in page_text])\n",
    "      joined_page_text = joined_page_text.replace('\"', \"'\")\n",
    "      \n",
    "      joined_html_code = ''.join(page_text_with_tags)\n",
    "      joined_html_code = joined_html_code.replace('\"', \"'\")\n",
    "\n",
    "      soupy_text = soup(joined_html_code, 'html.parser')\n",
    "\n",
    "      return joined_page_text, joined_html_code, last_updated_entry\n",
    "    \n",
    "\n",
    "\n",
    "    ## or, if there are divs on the page\n",
    "    else:\n",
    "      print('YEA, WE GOT DIVS IN THE BODY OF THIS PAGE')\n",
    "\n",
    "      # # HANDLE THE 'LAST UPDATED' FIELD\n",
    "      last_updated_entry = ['No entry located']\n",
    "      \n",
    "      entry = False\n",
    "\n",
    "      regex = r'(.ast .pdated)'\n",
    "      regex2 = r'(.ast .odified)'\n",
    "      regex3 = r'(.olicy .ffective)'\n",
    "      regex4 = r'(.ffective .ate)'\n",
    "      regex5 = r'(.olicy .odified)'\n",
    "      regex6 = r'(.pdated:)'\n",
    "      regex7 = r'(.ffective:)'\n",
    "\n",
    "\n",
    "    # for index, tag in enumerate(response.css('*')):\n",
    "      # for tag in response.css('*'):\n",
    "      for tag in response.css('body *'):\n",
    "        # print('LAST UPDATED ENTRY: ', last_updated_entry[0])\n",
    "        if last_updated_entry[0] != 'No entry located':\n",
    "          break\n",
    "\n",
    "        # print(tag)\n",
    "\n",
    "        else:\n",
    "          for pattern in [regex, regex2, regex3, regex4, regex5, regex6, regex7]:\n",
    "            if re.search(pattern, str(tag.css('*::text').get())):\n",
    "              print('HOW MANY TEXT ITEMS IN THIS TAG: ', len(tag.css('*::text').getall()))\n",
    "\n",
    "              if len(tag.css('*::text').getall()) > 1:\n",
    "                # print(''.join(tag.css('*::text').getall()))\n",
    "                # last_updated_entry.append(''.join(tag.css('*::text').getall()))\n",
    "                last_updated_entry[0] = ''.join(tag.css('*::text').getall())\n",
    "                break\n",
    "              \n",
    "      \n",
    "\n",
    "      if last_updated_entry[0] != 'No entry located':\n",
    "        update_entry = last_updated_entry[0]\n",
    "        update_entry = update_entry.split(':')[1].strip()\n",
    "        print('UPDATED ENTRY: ', update_entry)\n",
    "\n",
    "      else:\n",
    "        update_entry = last_updated_entry[0].strip()\n",
    "        print('UPDATED ENTRY: ', update_entry)\n",
    "\n",
    "\n",
    "\n",
    "      print('DO IT GOT A MAIN THO?', '\\n',len(response.css('body main').getall()), \n",
    "                                      '\\n', response.css('body main').getall())\n",
    "\n",
    "      if len(response.css('body main').getall()) > 0:\n",
    "        print('WE HAVE A MAIN TAG IN THE BODY')\n",
    "\n",
    "\n",
    "        # # NOW WE BEGIN THE PROCESS OF IDENTIFYING THE MAIN TEXT SECTION\n",
    "        for index,tag in enumerate(response.css('body *')): \n",
    "          # if index != 0:\n",
    "          div_h1_dict[index] = len(tag.css('h1'))\n",
    "          div_h2_dict[index] = len(tag.css('h2'))\n",
    "          div_h3_dict[index] = len(tag.css('h3'))\n",
    "          div_strong_dict[index] = len(tag.css('strong'))\n",
    "\n",
    "\n",
    "        for key, value in div_h1_dict.items():\n",
    "          if value == max(div_h1_dict.values()): \n",
    "            max_div_h1_index.append(key) \n",
    "\n",
    "        if max(div_h1_dict.values()) == 0:\n",
    "          max_div_h1_index.clear()\n",
    "          # print('NO H1 Headers in Divs on this webpage')\n",
    "\n",
    "        for key, value in div_h2_dict.items():\n",
    "          if value == max(div_h2_dict.values()): \n",
    "            max_div_h2_index.append(key) \n",
    "        if max(div_h2_dict.values()) == 0:\n",
    "          max_div_h2_index.clear()\n",
    "          # print('NO H2 Headers in Divs on this webpage')\n",
    "\n",
    "        for key, value in div_h3_dict.items():\n",
    "          if value == max(div_h3_dict.values()): \n",
    "            max_div_h3_index.append(key) \n",
    "        if max(div_h3_dict.values()) == 0:\n",
    "          max_div_h3_index.clear()\n",
    "          # print('NO H3 Headers in Divs on this webpage')\n",
    "\n",
    "        for key, value in div_strong_dict.items():\n",
    "          if value == max(div_strong_dict.values()): \n",
    "            max_div_strong_index.append(key) \n",
    "        if max(div_strong_dict.values()) == 0:\n",
    "          max_div_strong_index.clear()\n",
    "          # print('NO strong text in Divs on this webpage')\n",
    "\n",
    "\n",
    "        if len(max_div_h1_index) == 0:\n",
    "          if len(max_div_h2_index) > 0 & len(max_div_h3_index) > 0:\n",
    "            common_index = list(set(max_div_h2_index) & set(max_div_h3_index)) \n",
    "          elif len(max_div_h2_index) == 0 & len(max_div_h3_index) == 0:\n",
    "            common_index = max_div_strong_index\n",
    "          elif len(max_div_h2_index) == 0:\n",
    "            common_index = max_div_h3_index\n",
    "          else:\n",
    "            common_index = max_div_h2_index\n",
    "\n",
    "        else:\n",
    "          if len(max_div_h3_index) > 0:\n",
    "            print('WE HAVE H1 and H3')\n",
    "            if len(max_div_h2_index) > 0:\n",
    "              print('WE HAVE H2 TOO')\n",
    "              common_index = list(set(max_div_h1_index) & set(max_div_h2_index) & set(max_div_h3_index))\n",
    "              print('YEEERRRR COMMON INDEX OF D1, D2, AND D3:', common_index)\n",
    "              if len(common_index) == 0:\n",
    "                common_index = list(set(max_div_h1_index) & set(max_div_h3_index))\n",
    "                print('YEEERRRR COMMON INDEX OF D1 AND D3:', common_index)\n",
    "                if len(common_index) == 0:\n",
    "                  common_index = list(set(max_div_h1_index) & set(max_div_h2_index))\n",
    "                  print('YEEERRRR COMMON INDEX OF D1 AND D2:', common_index)\n",
    "                  if len(common_index) == 0:\n",
    "                    common_index = list(set(max_div_h2_index) & set(max_div_h3_index))\n",
    "                    print('YEEERRRR COMMON INDEX OF D2 AND D3:', common_index)\n",
    "\n",
    "            else:\n",
    "              print('WE AINT GOT NO H2!')\n",
    "              common_index = list(set(max_div_h1_index) & set(max_div_h3_index))\n",
    "              print('YEEERRRR COMMON INDEX OF D1 AND D3:', common_index)\n",
    "              if len(common_index) == 0:\n",
    "                common_index = list(set(max_div_h1_index) & set(max_div_strong_index))\n",
    "                print('YEEERRRR COMMON INDEX OF D1 AND STRONG:', common_index)\n",
    "                  \n",
    "\n",
    "\n",
    "          else:\n",
    "            print('WE HAVE H1 BUT NO H3')\n",
    "\n",
    "            if len(max_div_h2_index) > 0:\n",
    "              print('WE HAVE H2 THO')\n",
    "              common_index = list(set(max_div_h1_index) & set(max_div_h2_index))\n",
    "              print('YEEERRRR COMMON INDEX OF D1 AND D2:', common_index)\n",
    "              if len(common_index) == 0:\n",
    "                common_index = list(set(max_div_h1_index) & set(max_div_strong_index))\n",
    "                print('YEEERRRR COMMON INDEX OF D1 AND STRONG:', common_index)\n",
    "            else:\n",
    "              print('WE AINT GOT NO H2 EITHER!')\n",
    "              common_index = list(set(max_div_h1_index) & set(max_div_strong_index))\n",
    "              print('YEEERRRR COMMON INDEX OF D1 AND STRONG:', common_index)\n",
    "\n",
    "\n",
    "\n",
    "        # elif len(max_div_h3_index) > 0 & len(max_div_h2_index) > 0:\n",
    "        #   common_index = list(set(max_div_h1_index) & set(max_div_h2_index) & set(max_div_h3_index))\n",
    "        #   print('YEEERRRR COMMON INDEX OF D1, D2, AND D3:', common_index)\n",
    "        #   if len(common_index) == 0:\n",
    "        #     common_index = list(set(max_div_h1_index) & set(max_div_h3_index))\n",
    "        #     print('YEEERRRR COMMON INDEX OF D1 AND D3:', common_index)\n",
    "        #     if len(common_index) == 0:\n",
    "        #       common_index = list(set(max_div_h1_index) & set(max_div_h2_index))\n",
    "        #       print('YEEERRRR COMMON INDEX OF D1 AND D2:', common_index)\n",
    "        #       if len(common_index) == 0:\n",
    "        #         common_index = list(set(max_div_h2_index) & set(max_div_h3_index))\n",
    "        #         print('YEEERRRR COMMON INDEX OF D2 AND D3:', common_index)\n",
    "\n",
    "\n",
    "        # elif len(max_div_h3_index) == 0:\n",
    "        #   if len(max_div_h2_index) > 0:\n",
    "        #     common_index = list(set(max_div_h1_index) & set(max_div_h2_index))\n",
    "        #   else:\n",
    "        #     common_index = max_div_strong_index\n",
    "\n",
    "        # elif len(max_div_h2_index) == 0:\n",
    "        #   if len(max_div_h3_index) > 0:\n",
    "        #     common_index = list(set(max_div_h1_index) & set(max_div_h3_index))\n",
    "        #   else:\n",
    "        #     common_index = max_div_strong_index\n",
    "          \n",
    "        # else:\n",
    "        #   common_index = list(set(max_div_h1_index) & set(max_div_h2_index) & set(max_div_h3_index)) \n",
    "\n",
    "        print('H1:', max_div_h1_index)\n",
    "        print(max_div_h2_index)\n",
    "        print(max_div_h3_index)\n",
    "        print('strong:', max_div_strong_index)\n",
    "        print('COMMON INDEX:', common_index)\n",
    "\n",
    "        # print(response.css('body div')[min(max_div_h1_index)])\n",
    "        # print('DIV TEXT FROM LUCKY DIV:', response.css('body div').getall()[min(common_index)])\n",
    "\n",
    "        key_index = min(common_index)\n",
    "\n",
    "        # if len(common_index) > 0:\n",
    "        #   # print(common_index)\n",
    "        #   key_index = common_index[0]\n",
    "        # elif len(max_div_h2_index) > 0:\n",
    "        #   # print(max(max_div_h2_index))\n",
    "        #   key_index = max_div_h2_index[0]\n",
    "        # elif len(max_div_h1_index) > 0:\n",
    "        #   # print(min(max_div_h1_index))\n",
    "        #   key_index = max_div_h1_index[0]\n",
    "        # elif len(max_div_h3_index) > 0:\n",
    "        #   # print(max(max_div_h3_index))\n",
    "        #   key_index = max_div_h3_index[0]\n",
    "        # elif len(max_div_strong_index) > 0:\n",
    "        #   # print(max(max_div_strong_index))\n",
    "        #   key_index = max_div_strong_index[0]\n",
    "        # else:\n",
    "        #   print('THIS WEBSITE IS FORMATTED REALLY WEIRDLY')\n",
    "\n",
    "        lucky_tag = response.css('body *')[key_index]\n",
    "\n",
    "      else:\n",
    "        print('NO MAIN TAG IN THE BODY')\n",
    "\n",
    "        # # NOW WE BEGIN THE PROCESS OF IDENTIFYING THE MAIN TEXT SECTION\n",
    "        for index,tag in enumerate(response.css('body div')): \n",
    "          # if index != 0:\n",
    "          div_h1_dict[index] = len(tag.css('h1'))\n",
    "          div_h2_dict[index] = len(tag.css('h2'))\n",
    "          div_h3_dict[index] = len(tag.css('h3'))\n",
    "          div_strong_dict[index] = len(tag.css('strong'))\n",
    "\n",
    "\n",
    "        for key, value in div_h1_dict.items():\n",
    "          if value == max(div_h1_dict.values()): \n",
    "            max_div_h1_index.append(key) \n",
    "\n",
    "        if max(div_h1_dict.values()) == 0:\n",
    "          max_div_h1_index.clear()\n",
    "          # print('NO H1 Headers in Divs on this webpage')\n",
    "\n",
    "        for key, value in div_h2_dict.items():\n",
    "          if value == max(div_h2_dict.values()): \n",
    "            max_div_h2_index.append(key) \n",
    "        if max(div_h2_dict.values()) == 0:\n",
    "          max_div_h2_index.clear()\n",
    "          # print('NO H2 Headers in Divs on this webpage')\n",
    "\n",
    "        for key, value in div_h3_dict.items():\n",
    "          if value == max(div_h3_dict.values()): \n",
    "            max_div_h3_index.append(key) \n",
    "        if max(div_h3_dict.values()) == 0:\n",
    "          max_div_h3_index.clear()\n",
    "          # print('NO H3 Headers in Divs on this webpage')\n",
    "\n",
    "        for key, value in div_strong_dict.items():\n",
    "          if value == max(div_strong_dict.values()): \n",
    "            max_div_strong_index.append(key) \n",
    "        if max(div_strong_dict.values()) == 0:\n",
    "          max_div_strong_index.clear()\n",
    "          # print('NO strong text in Divs on this webpage')\n",
    "\n",
    "\n",
    "\n",
    "        if len(max_div_h1_index) == 0:\n",
    "          print('WE HAVE NO H1')\n",
    "          if len(max_div_h2_index) > 0 & len(max_div_h3_index) > 0:\n",
    "            common_index = list(set(max_div_h2_index) & set(max_div_h3_index)) \n",
    "          elif len(max_div_h2_index) == 0 & len(max_div_h3_index) == 0:\n",
    "            common_index = max_div_strong_index\n",
    "          elif len(max_div_h2_index) == 0:\n",
    "            common_index = max_div_h3_index\n",
    "          else:\n",
    "            common_index = max_div_h2_index\n",
    "\n",
    "        else:\n",
    "          if len(max_div_h3_index) > 0:\n",
    "            print('WE HAVE H1 and H3')\n",
    "            if len(max_div_h2_index) > 0:\n",
    "              print('WE HAVE H2 TOO')\n",
    "              common_index = list(set(max_div_h1_index) & set(max_div_h2_index) & set(max_div_h3_index))\n",
    "              print('YEEERRRR COMMON INDEX OF D1, D2, AND D3:', common_index)\n",
    "              if len(common_index) == 0:\n",
    "                common_index = list(set(max_div_h1_index) & set(max_div_h3_index))\n",
    "                print('YEEERRRR COMMON INDEX OF D1 AND D3:', common_index)\n",
    "                if len(common_index) == 0:\n",
    "                  common_index = list(set(max_div_h1_index) & set(max_div_h2_index))\n",
    "                  print('YEEERRRR COMMON INDEX OF D1 AND D2:', common_index)\n",
    "                  if len(common_index) == 0:\n",
    "                    common_index = list(set(max_div_h2_index) & set(max_div_h3_index))\n",
    "                    print('YEEERRRR COMMON INDEX OF D2 AND D3:', common_index)\n",
    "\n",
    "            else:\n",
    "              print('WE AINT GOT NO H2!')\n",
    "              common_index = list(set(max_div_h1_index) & set(max_div_h3_index))\n",
    "              print('YEEERRRR COMMON INDEX OF D1 AND D3:', common_index)\n",
    "              if len(common_index) == 0:\n",
    "                common_index = list(set(max_div_h1_index) & set(max_div_strong_index))\n",
    "                print('YEEERRRR COMMON INDEX OF D1 AND STRONG:', common_index)\n",
    "                  \n",
    "\n",
    "\n",
    "          else:\n",
    "            print('WE HAVE H1 BUT NO H3')\n",
    "\n",
    "            if len(max_div_h2_index) > 0:\n",
    "              print('WE HAVE H2 THO')\n",
    "              common_index = list(set(max_div_h1_index) & set(max_div_h2_index))\n",
    "              print('YEEERRRR COMMON INDEX OF D1 AND D2:', common_index)\n",
    "              if len(common_index) == 0:\n",
    "                common_index = list(set(max_div_h1_index) & set(max_div_strong_index))\n",
    "                print('YEEERRRR COMMON INDEX OF D1 AND STRONG:', common_index)\n",
    "            else:\n",
    "              print('WE AINT GOT NO H2 EITHER!')\n",
    "              common_index = list(set(max_div_h1_index) & set(max_div_strong_index))\n",
    "              print('YEEERRRR COMMON INDEX OF D1 AND STRONG:', common_index)\n",
    "        # if len(max_div_h1_index) == 0:\n",
    "        #   if len(max_div_h2_index) > 0 & len(max_div_h3_index) > 0:\n",
    "        #     common_index = list(set(max_div_h2_index) & set(max_div_h3_index)) \n",
    "        #   elif len(max_div_h2_index) == 0 & len(max_div_h3_index) == 0:\n",
    "        #     common_index = max_div_strong_index\n",
    "        #   elif len(max_div_h2_index) == 0:\n",
    "        #     common_index = max_div_h3_index\n",
    "        #   else:\n",
    "        #     common_index = max_div_h2_index\n",
    "\n",
    "\n",
    "        # elif len(max_div_h3_index) == 0:\n",
    "        #   common_index = list(set(max_div_h1_index) & set(max_div_h2_index))\n",
    "\n",
    "        # elif len(max_div_h2_index) == 0:\n",
    "        #   common_index = list(set(max_div_h1_index) & set(max_div_h3_index))\n",
    "        #   print('COMMON INDEX OF D1 and D3:', common_index)\n",
    "        # else:\n",
    "        #   common_index = list(set(max_div_h1_index) & set(max_div_h2_index) & set(max_div_h3_index)) \n",
    "\n",
    "        print('H1:', max_div_h1_index)\n",
    "        print(max_div_h2_index)\n",
    "        print(max_div_h3_index)\n",
    "        print('strong:', max_div_strong_index)\n",
    "        print('COMMON INDEX:', common_index)\n",
    "\n",
    "        # print(response.css('body div')[min(max_div_h1_index)])\n",
    "        # print('DIV TEXT FROM LUCKY DIV:', response.css('body div').getall()[min(common_index)])\n",
    "\n",
    "        key_index = min(common_index)\n",
    "\n",
    "        # if len(common_index) > 0:\n",
    "        #   # print(common_index)\n",
    "        #   key_index = common_index[0]\n",
    "        # elif len(max_div_h2_index) > 0:\n",
    "        #   # print(max(max_div_h2_index))\n",
    "        #   key_index = max_div_h2_index[0]\n",
    "        # elif len(max_div_h1_index) > 0:\n",
    "        #   # print(min(max_div_h1_index))\n",
    "        #   key_index = max_div_h1_index[0]\n",
    "        # elif len(max_div_h3_index) > 0:\n",
    "        #   # print(max(max_div_h3_index))\n",
    "        #   key_index = max_div_h3_index[0]\n",
    "        # elif len(max_div_strong_index) > 0:\n",
    "        #   # print(max(max_div_strong_index))\n",
    "        #   key_index = max_div_strong_index[0]\n",
    "        # else:\n",
    "        #   print('THIS WEBSITE IS FORMATTED REALLY WEIRDLY')\n",
    "\n",
    "        lucky_tag = response.css('body div')[key_index]\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "      page_text_with_tags = ['start']\n",
    "      page_text = ['start']            \n",
    "\n",
    "      first_header_index = []\n",
    "      for index,tag in enumerate(lucky_tag.css('*')):\n",
    "        if len(first_header_index) > 0:\n",
    "          break\n",
    "\n",
    "        for header in ['<h1', '<h2', '<h3', '<h4', '<h5']:\n",
    "          if header in tag.get()[0:3]:\n",
    "            first_header_index.append(index)\n",
    "            print('OBTAINED DANK HEADER')\n",
    "            break\n",
    "          else:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "      for index,tag in enumerate(lucky_tag.css('*')[first_header_index[0]:]):\n",
    "        if index == 0:\n",
    "          print('OUTERMOST OBJECT INSIDE LUCKY TAG')\n",
    "          print(tag.css('* ::text').getall())\n",
    "\n",
    "\n",
    "        if '<footer' in tag.get()[0:7]:\n",
    "          break\n",
    "\n",
    "        if '<script' not in tag.get() and '<style' not in tag.get() and '<option' not in tag.get():\n",
    "  \n",
    "\n",
    "          if tag.get() not in page_text_with_tags[-1]:\n",
    "            \n",
    "            ## this one still has the html wrappers within the outside tag\n",
    "            page_text_with_tags.append(tag.get())\n",
    "\n",
    "            # for index, code in enumerate(page_text_with_tags):\n",
    "              # print('tag index', index)\n",
    "              # print('tag code', '\\n', code, '\\n')\n",
    "\n",
    "\n",
    "            if '<a' in tag.get():\n",
    "              ## this one takes the text out of each tag of a layered tag and joins them\n",
    "              joined_tag_text = ' '.join(tag.css('*::text').getall())\n",
    "              page_text.append(joined_tag_text)\n",
    "\n",
    "            else:\n",
    "              ## this one takes the text out of each tag\n",
    "              page_text.append(tag.css('*::text').getall())\n",
    "\n",
    "\n",
    "      del(page_text_with_tags[0])\n",
    "      del(page_text[0])\n",
    "\n",
    "     \n",
    "      joined_page_text = []\n",
    "\n",
    "      for index,text_list in enumerate(page_text):\n",
    "        # print('Tag index: ', index)\n",
    "        # print('Number of text objects in tag: ', len(text_list))\n",
    "        # print('Tag chunk: ', tag.get(), '\\n', '\\n')\n",
    "\n",
    "        joined_page_text.append(''.join(text_list))\n",
    "\n",
    "      ## outer join of text chunks\n",
    "      joined_page_text = '\\n'.join(joined_page_text)\n",
    "      joined_page_text = joined_page_text.replace('\"', \"'\")\n",
    "      \n",
    "      # joined_html_code = ''.join(page_text_with_tags)\n",
    "      # joined_html_code = joined_html_code.replace('\"', \"'\")\n",
    "\n",
    "\n",
    "      # soupy_text = soup(joined_html_code, 'html.parser')\n",
    "      # print(soupy_text.prettify())\n",
    "\n",
    "      # return joined_page_text, joined_html_code, last_updated_entry\n",
    "      return joined_page_text, update_entry\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # used for cases like Twitter where the initial website was attempted to be redirected\n",
    "        # the initial 302 http status code attempted to redirect the twitter home page to a mobile version\n",
    "        # however, the mobile version causes a 404 error and cannot be scraped\n",
    "          # so, we want to prevent the 302 redirection to the mobile page\n",
    "  # def make_requests_from_url(self, url):\n",
    "  #       return scrapy.Request(url, dont_filter=True, meta = {\n",
    "  #                 'dont_redirect': True,\n",
    "  #                 'handle_httpstatus_list': [301,302]\n",
    "  #           })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  # the parse method is run on the response, and is a required method for the spider\n",
    "    #this method will get repeated for all the relevant links on the page\n",
    "  def parse(self, response):\n",
    "\n",
    "    self.logger.debug(\"CURRENT USER AGENT: {}\".format(response.request.headers['User-Agent']))\n",
    "    self.logger.debug(\"(parse) response: status=%d, URL=%s\" % (response.status, response.url))\n",
    "    self.logger.info('Parse function called on %s', response.url)\n",
    "\n",
    "\n",
    "\n",
    "    self.domain = {}\n",
    "    self.domain[response.url] = {}\n",
    "    self.domain[response.url]['privacy'] = []\n",
    "    self.domain[response.url]['terms'] = []\n",
    "    self.domain[response.url]['conditions'] = []\n",
    "    self.domain[response.url]['cookies'] = [] \n",
    "\n",
    "\n",
    "    print('\\n', '------------- STARTING PARSE OF BASE URL -------------')\n",
    "    print('OUTER OBJECT:', self.domain)\n",
    "\n",
    "\n",
    "    links_items_return = self.scrape_interesting_links(response)\n",
    "\n",
    "    print('\\n', '------------- BACK IN PARSE NOW -------------')\n",
    "\n",
    "    \n",
    "    next_links = links_items_return[0] \n",
    "    inner_object = links_items_return[1]\n",
    "    items_list = links_items_return[2]\n",
    "\n",
    "    print('NEXT LINKS', next_links)\n",
    "    print('INNER OBJECT', inner_object, '\\n')\n",
    "    print('ITEMS LIST', items_list, '\\n')\n",
    "\n",
    "\n",
    "    # # using length of items list to determine pathway for the category\n",
    "    for entry in items_list:\n",
    "      print('\\n', 'ENTRY: ', entry)\n",
    "\n",
    "      if len(entry) == 0:\n",
    "        print('NO ITEMS FOR THIS CATEGORY')\n",
    "\n",
    "      elif len(entry) == 1:\n",
    "        print('ONE ITEM FOR THIS CATEGORY')\n",
    "        next_page = response.urljoin(entry[0]['link'])\n",
    "        print('ABOUT TO BE SCRAPED:', next_page)\n",
    "        yield scrapy.Request(next_page, callback=self.rule_parse2)\n",
    "\n",
    "\n",
    "      else:\n",
    "        print('MULTIPLE ITEMS FOR THIS CATEGORY')\n",
    "        print('LENGTH OF ENTRY: ', len(entry))\n",
    "        for item in entry:\n",
    "          next_page = response.urljoin(item['link'])\n",
    "          print('NEXT PAGE TO BE PROCESSED: ', next_page)\n",
    "          yield scrapy.Request(next_page, callback=self.reduce_category)\n",
    "\n",
    "\n",
    "    # print('INNER OBJECT ITEMS', '\\n', list(inner_object.items()))\n",
    "    # for item in inner_object.items():\n",
    "    #   print('CATEGORY:   ', item[0])\n",
    "    #   print('ITEMS:   ', item[1])\n",
    "\n",
    "\n",
    "    #   if len(item[1]) == 0:\n",
    "    #     pass\n",
    "\n",
    "    #   elif len(item[1]) == 1:\n",
    "    #     next_page = response.urljoin(item[1][0]['link'])\n",
    "    #     print('ABOUT TO BE SCRAPED:', next_page)\n",
    "    #     yield scrapy.Request(next_page, callback=self.rule_parse2)\n",
    "\n",
    "    #   else:\n",
    "    #     for each in item[1]:\n",
    "    #       print('each object inside a category')\n",
    "    #       print(each)\n",
    "\n",
    "    #       next_page = response.urljoin(each['link'])\n",
    "    #       print('ABOUT TO BE PROCESSED:', next_page)\n",
    "    #       yield scrapy.Request(next_page, callback=self.reduce_category)\n",
    "\n",
    "          # # I don't think this is needed because I reduce each category in the previous method\n",
    "          # if each['link'] in next_links:\n",
    "          #  pass\n",
    "\n",
    "\n",
    "  def reduce_category(self, response):\n",
    "    print('\\n', '-------------- INSIDE OF REDUCE CATEGORY FUNCTION --------------')\n",
    "\n",
    "    print('CURRENT PATHWAY: ', response.url)\n",
    "\n",
    "    print('SITE: ', response.css('body *::text').get())\n",
    "    print('SITE COUNT: ', len(response.css('body').re(r'site')))\n",
    "\n",
    "\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  def rule_parse2(self, response):\n",
    "    print('\\n', '-------------- INSIDE OF RULE PARSE 2!! --------------')\n",
    "    print('PAGE TITLE: ', response.css('title::text').get())\n",
    "    print('PAGE URL: ', response.url)\n",
    "\n",
    "\n",
    "    # load_ins = self.domain.keys()\n",
    "    # print('LOAD IN', load_ins)\n",
    "\n",
    "    base_url = list(self.domain.keys())[0]\n",
    "\n",
    "    print('INNER OBJECT \"self.domain[base_url]\": ', '\\n', self.domain[base_url])\n",
    "\n",
    "\n",
    "    if len(response.css('title').re(r'.rivacy')) != 0:\n",
    "\n",
    "      if (len(response.css('title').re(r'.tatement')) != 0) | (len(response.css('title').re(r'.olicy')) != 0) | (len(response.css('title').re(r'.otice')) != 0):\n",
    "\n",
    "        policy_scraper_results = self.policy_scraper(response)\n",
    "        joined_page_text = policy_scraper_results[0]\n",
    "        last_updated_entry = policy_scraper_results[1]\n",
    "\n",
    "\n",
    "        # joined_page_text = self.policy_scraper(response)[0]\n",
    "        # # soupy_text = self.policy_scraper(response)[1]\n",
    "        # last_updated_entry = self.policy_scraper(response)[1]\n",
    "\n",
    "        # privacy_item = self.domain['link_dict']['privacy']\n",
    "        privacy_item = self.domain[base_url]['privacy'][0]\n",
    "        print('PRIVACY ITEM LOADED FROM SELF:', privacy_item)\n",
    "\n",
    "\n",
    "        privacy_item['title'] = response.css('title::text').get()\n",
    "        privacy_item['url'] = response.url\n",
    "        privacy_item['last_updated'] = last_updated_entry\n",
    "        privacy_item['text'] = joined_page_text\n",
    "        # privacy_item['html'] = soupy_text\n",
    "\n",
    "        # self.domain['link_dict']['privacy'] = privacy_item\n",
    "        self.domain[base_url]['privacy'] = privacy_item\n",
    "\n",
    "      else:\n",
    "        print('LINKS ON THIS PAGE:')\n",
    "        for link in response.css('a').getall():\n",
    "          print(link)\n",
    "\n",
    "\n",
    "    ## REPEAT ABOVE PROCESS FOR OTHER TYPES OF POLICIES\n",
    "\n",
    "    if len(response.css('title').re(r'.erms')) != 0:\n",
    "\n",
    "      policy_scraper_results = self.policy_scraper(response)\n",
    "      joined_page_text = policy_scraper_results[0]\n",
    "      last_updated_entry = policy_scraper_results[1]\n",
    "\n",
    "      # joined_page_text = self.policy_scraper(response)[0]\n",
    "      # # soupy_text = self.policy_scraper(response)[1]\n",
    "      # last_updated_entry = self.policy_scraper(response)[1]\n",
    "\n",
    "      # terms_item = self.domain['link_dict']['terms']\n",
    "      terms_item = self.domain[base_url]['terms'][0]\n",
    "\n",
    "      terms_item['title'] = response.css('title::text').get()\n",
    "      terms_item['url'] = response.url\n",
    "      terms_item['last_updated'] = last_updated_entry\n",
    "      terms_item['text'] = joined_page_text\n",
    "      # terms_item['html'] = soupy_text\n",
    "\n",
    "      # self.domain['link_dict']['terms'] = terms_item\n",
    "      self.domain[base_url]['terms'] = terms_item\n",
    "\n",
    "\n",
    "\n",
    "    if len(response.css('title').re(r'.onditions')) != 0:\n",
    "\n",
    "      if len(response.css('title').re(r'.erms')) == 0:\n",
    "\n",
    "        policy_scraper_results = self.policy_scraper(response)\n",
    "        joined_page_text = policy_scraper_results[0]\n",
    "        last_updated_entry = policy_scraper_results[1]\n",
    "\n",
    "        # joined_page_text = self.policy_scraper(response)[0]\n",
    "        # # soupy_text = self.policy_scraper(response)[1]\n",
    "        # last_updated_entry = self.policy_scraper(response)[1]\n",
    "\n",
    "        # conditions_item = self.domain['link_dict']['conditions']\n",
    "        conditions_item = self.domain[base_url]['conditions'][0]\n",
    "\n",
    "        conditions_item['title'] = response.css('title::text').get()\n",
    "        conditions_item['url'] = response.url\n",
    "        conditions_item['last_updated'] = last_updated_entry\n",
    "        conditions_item['text'] = joined_page_text\n",
    "        # conditions_item['html'] = soupy_text\n",
    "\n",
    "        # self.domain['link_dict']['conditions'] = conditions_item\n",
    "        self.domain[base_url]['conditions'] = conditions_item\n",
    "\n",
    "\n",
    "\n",
    "    if len(response.css('title').re(r'.ookie')) != 0:\n",
    "\n",
    "      policy_scraper_results = self.policy_scraper(response)\n",
    "      joined_page_text = policy_scraper_results[0]\n",
    "      last_updated_entry = policy_scraper_results[1]\n",
    "\n",
    "      # joined_page_text = self.policy_scraper(response)[0]\n",
    "      # # soupy_text = self.policy_scraper(response)[1]\n",
    "      # last_updated_entry = self.policy_scraper(response)[1]\n",
    "\n",
    "      # conditions_item = self.domain['link_dict']['conditions']\n",
    "      cookies_item = self.domain[base_url]['cookies'][0]\n",
    "\n",
    "      cookies_item['title'] = response.css('title::text').get()\n",
    "      cookies_item['url'] = response.url\n",
    "      cookies_item['last_updated'] = last_updated_entry\n",
    "      cookies_item['text'] = joined_page_text\n",
    "      # conditions_item['html'] = soupy_text\n",
    "\n",
    "      # self.domain['link_dict']['conditions'] = conditions_item\n",
    "      self.domain[base_url]['cookies'] = cookies_item\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # print('LINK DICT BEFORE YIELD:', self.domain[base_url])\n",
    "\n",
    "\n",
    "    length_list = []\n",
    "\n",
    "    print('POLICY ITEMS:', list(self.domain[base_url].keys()))\n",
    "\n",
    "    for index, key in enumerate(list(self.domain[base_url].keys())):\n",
    "      print('INDEX: ', index)\n",
    "      print('LENGTH OF VALUE: ', len(self.domain[base_url][key]))\n",
    "      \n",
    "      length_list.append(len(self.domain[base_url][key]))\n",
    "\n",
    "      # item_tuple = (index, len(self.domain[base_url][key]))\n",
    "      # length_list.append(item_tuple)\n",
    "\n",
    "      # if len(self.domain[base_url][key]) != 0:\n",
    "      #   if len(self.domain[base_url][key]) == 5:\n",
    "      #     print('THIS CATEGORY IS COMPLETE')\n",
    "\n",
    "    \n",
    "    all_policy_names = []\n",
    "    all_policy_urls = []\n",
    "    all_policy_text = []\n",
    "    all_update_entries = []\n",
    "\n",
    "    new_dates_list = []\n",
    "\n",
    "    if (length_list.count(0) + length_list.count(5) != len(length_list)):\n",
    "      print('WE CAN NOT YIELD THE ITEM YET')\n",
    "      yield\n",
    "    else:\n",
    "      print('ALL CATEGORIES ARE COMPLETE... WE CAN YIELD THE ITEM AND FINISH')\n",
    "      for key in self.domain[base_url].keys():\n",
    "        if len(self.domain[base_url][key]) > 0:\n",
    "          print('URL OF ITEM: ', self.domain[base_url][key]['url'])\n",
    "\n",
    "          all_policy_text.append(self.domain[base_url][key]['text'])\n",
    "          all_update_entries.append(self.domain[base_url][key]['last_updated'])\n",
    "          all_policy_names.append(self.domain[base_url][key]['title'])\n",
    "          all_policy_urls.append(self.domain[base_url][key]['url'])\n",
    "\n",
    "\n",
    "        # print('URL OF ITEM: ', self.domain[base_url][key]['url'])\n",
    "\n",
    "      if all_update_entries.count('No entry located') == len(all_update_entries):\n",
    "        all_update_entries = 'No entry located'\n",
    "        most_recent_entry = 'No entry located'\n",
    "\n",
    "      elif all_update_entries.count('No entry located') > 0:\n",
    "        print('AT LEAST ONE NO ENTRY LOCATED VALUE')\n",
    "        for index, entry in enumerate(all_update_entries):\n",
    "          if entry == 'No entry located':\n",
    "            del(all_update_entries[index])\n",
    "\n",
    "        # # here we do datetime stuff to find the most recent one\n",
    "          else:\n",
    "            print('ALL UPDATE ENTRIES:', all_update_entries)\n",
    "            print('UPDATED ENTRIES AT THIS INDEX:', all_update_entries[index])\n",
    "            print('TYPE OF UPDATED ENTRIES AT THIS INDEX:', type(all_update_entries[index]))\n",
    "            print('CONVERTING TO DATE FORMAT IN HERE')\n",
    "            print('ENTRY', entry)\n",
    "            \n",
    "            new_date = self.validate(entry)\n",
    "            print('NEW DATE: ', new_date)\n",
    "            print('TYPE OF NEW DATE: ', type(new_date))\n",
    "            new_dates_list.append(new_date)\n",
    "\n",
    "        print('NEW DATES LIST:', new_dates_list)\n",
    "        print('MOST RECENT DATE:', max(new_dates_list))\n",
    "\n",
    "        most_recent_entry = max(new_dates_list)\n",
    "        print('IS MOST RECENT ENTRY MORE RECENT THAN NOW: ', most_recent_entry > datetime.datetime.now())\n",
    "\n",
    "\n",
    "      else:\n",
    "        # # this is if there are zero 'no entry located' values\n",
    "        # # do datetime stuff here too\n",
    "        print('CONVERTING TO DATE FORMAT IN HEEEEERE')\n",
    "\n",
    "        for entry in all_update_entries:\n",
    "          new_date = self.validate(entry)\n",
    "          print('NEW DATE: ', new_date)\n",
    "          print('TYPE OF NEW DATE: ', type(new_date))\n",
    "          new_dates_list.append(new_date)\n",
    "\n",
    "        print('NEW DATES LIST:', new_dates_list)\n",
    "        print('MOST RECENT DATE:', max(new_dates_list))\n",
    "\n",
    "        most_recent_entry = max(new_dates_list)\n",
    "\n",
    "        self.text = '\\n'.join(all_policy_text)\n",
    "        self.updates = most_recent_entry\n",
    "        self.titles = all_policy_names\n",
    "        self.urls = all_policy_urls\n",
    "\n",
    "      export_item = ExportItem()\n",
    "      export_item['text'] = '\\n'.join(all_policy_text)\n",
    "      export_item['updates'] = most_recent_entry\n",
    "      export_item['titles'] = all_policy_names\n",
    "      export_item['urls'] = all_policy_urls\n",
    "\n",
    "\n",
    "      yield export_item\n",
    "        \n",
    "        \n",
    "\n",
    "process = CrawlerProcess(get_project_settings())\n",
    "process.crawl(finalSpider, origin_url='https://www.amazon.com')\n",
    "process.start()\n",
    "      \n",
    "# print(\"Hi this worked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
